{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bad81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "import docs\n",
    "from spacy import displacy\n",
    "import docx\n",
    "import spacy\n",
    "from spacy import schemas\n",
    "from spacy import Dict\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import  STOP_WORDS\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import doc\n",
    "import textract\n",
    "import antiword\n",
    "from PyPDF2 import PdfFileReader\n",
    "import re\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "from spacy.matcher import Matcher\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "import doc\n",
    "import textract\n",
    "import antiword\n",
    "from PyPDF2 import PdfFileReader\n",
    "import re\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "from spacy.matcher import Matcher\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'calvin klein design dress calvin klein'\n",
    "\n",
    "def uniquify(string):\n",
    "    output = []\n",
    "    seen = set()\n",
    "    for word in string.split():\n",
    "        if word not in seen:\n",
    "            output.append(word)\n",
    "            seen.add(word)\n",
    "    return ' '.join(output)\n",
    "\n",
    "print(uniquify(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc920da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6195f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import glob\n",
    "import doc\n",
    "import textract\n",
    "import antiword\n",
    "from PyPDF2 import PdfFileReader\n",
    "import re\n",
    "import re\n",
    "import nltk\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95204760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import glob as glob\n",
    "path = './Resumes/workday resumes/'\n",
    "file_list= []\n",
    "for file in glob.glob(path+ \"/*.docx\"):\n",
    "    data_file_list = os.path.basename(file)\n",
    "    file_list.append(data_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f105d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChinnaSubbarayuduM_Hexaware.docx',\n",
       " 'Gopi Krishna_Hexaware.docx',\n",
       " 'Himaja G_(Hexaware).docx',\n",
       " 'Jyotiverma_Heaware.docx',\n",
       " 'MooraboyinaGuravaiah_Hexaware.docx',\n",
       " 'P V Sai Krishna_ Hexaware.docx',\n",
       " 'RahulM_Hexaware.docx',\n",
       " 'RameshP_Hexaware.docx',\n",
       " 'Srikanth-Hexaware.docx',\n",
       " 'SSKumar_Hexaware.docx',\n",
       " 'Vinay Kumar_Hexaware.docx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26584932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e41c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515aa4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df[\"cv\"]=file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e711c5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChinnaSubbarayuduM_Hexaware.docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gopi Krishna_Hexaware.docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Himaja G_(Hexaware).docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jyotiverma_Heaware.docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MooraboyinaGuravaiah_Hexaware.docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P V Sai Krishna_ Hexaware.docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RahulM_Hexaware.docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RameshP_Hexaware.docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Srikanth-Hexaware.docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SSKumar_Hexaware.docx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vinay Kumar_Hexaware.docx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    cv\n",
       "0     ChinnaSubbarayuduM_Hexaware.docx\n",
       "1           Gopi Krishna_Hexaware.docx\n",
       "2             Himaja G_(Hexaware).docx\n",
       "3              Jyotiverma_Heaware.docx\n",
       "4   MooraboyinaGuravaiah_Hexaware.docx\n",
       "5       P V Sai Krishna_ Hexaware.docx\n",
       "6                 RahulM_Hexaware.docx\n",
       "7                RameshP_Hexaware.docx\n",
       "8               Srikanth-Hexaware.docx\n",
       "9                SSKumar_Hexaware.docx\n",
       "10           Vinay Kumar_Hexaware.docx"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c74f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=[]\n",
    "for i in range (len(file_list)):\n",
    "    label=\"workday_resumes\"\n",
    "    label_list.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f5cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df[\"label\"]=label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c666714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChinnaSubbarayuduM_Hexaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gopi Krishna_Hexaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Himaja G_(Hexaware).docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jyotiverma_Heaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MooraboyinaGuravaiah_Hexaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P V Sai Krishna_ Hexaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RahulM_Hexaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RameshP_Hexaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Srikanth-Hexaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SSKumar_Hexaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vinay Kumar_Hexaware.docx</td>\n",
       "      <td>workday_resumes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    cv            label\n",
       "0     ChinnaSubbarayuduM_Hexaware.docx  workday_resumes\n",
       "1           Gopi Krishna_Hexaware.docx  workday_resumes\n",
       "2             Himaja G_(Hexaware).docx  workday_resumes\n",
       "3              Jyotiverma_Heaware.docx  workday_resumes\n",
       "4   MooraboyinaGuravaiah_Hexaware.docx  workday_resumes\n",
       "5       P V Sai Krishna_ Hexaware.docx  workday_resumes\n",
       "6                 RahulM_Hexaware.docx  workday_resumes\n",
       "7                RameshP_Hexaware.docx  workday_resumes\n",
       "8               Srikanth-Hexaware.docx  workday_resumes\n",
       "9                SSKumar_Hexaware.docx  workday_resumes\n",
       "10           Vinay Kumar_Hexaware.docx  workday_resumes"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e39a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtxt(filename):\n",
    "    doc = docx.Document(filename,)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f5e339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readpdf(filename):\n",
    "    pdf=pdfplumber.open(filename)\n",
    "    pages = pdf.pages[0]\n",
    "    \n",
    "    fullText = []\n",
    "    for para in pages.extract_text:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1619654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./Resumes/React_Developer/Reactjs Developer_Prabakaran_Musquare Technologies.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "230496f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mreadpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Resumes/React_Developer/Reactjs Developer_Prabakaran_Musquare Technologies.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36mreadpdf\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      3\u001b[0m pages \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mpages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m fullText \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m pages\u001b[38;5;241m.\u001b[39mextract_text:\n\u001b[0;32m      7\u001b[0m     fullText\u001b[38;5;241m.\u001b[39mappend(para\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(fullText)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "readpdf(\"./Resumes/React_Developer/Reactjs Developer_Prabakaran_Musquare Technologies.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51ac0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ef733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4a339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "36bdd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readpdf(filename):\n",
    "    pdf=pdfplumber.open(filename)\n",
    "    pages = pdf.pages[0]\n",
    "    text=pages.extract_text()\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "106f763a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     \\nName: M. Prabakaran \\nTitle: UI Developer \\n \\nPROFESSIONAL SUMMARY \\n \\n●  2.4+ years of Professional IT experience as a software developer having knowledge on different UI based \\nApplication. \\n●  Hands on experience in HTML, CSS, JS, ReactJS.   \\n●  Hands on experience in handling UI interaction, Design methodology. \\n●  Handling In-App purchase, uploading and maintaining apps in play store. \\n●  Hands on experience with customization over base-product depends on client requirement.  \\n●  Cohesive team worker, having strong analytical, problem solving and interpersonal skills. \\n \\nEDUCATION \\n \\n●  Completed on 2017 Bachelor of Technology (ECE), PRIST University, Tamil Nadu. \\n●  Completed on 2012 Higher Secondary, Mount Park Hr Sec School, Thiyagadurgam, Tamil Nadu. \\n●  Completed on 2010 SSLC, Krishnasamy Hr Sec School, Cuddalore, Tamil Nadu. \\n \\nSKILLS \\n \\nOperating System  Windows, Unix,Mac \\nProgramming Languages  HTML, CSS, JS, React \\nMobile App Development  Windows and Android Apps \\nDesigning  Constatinant layout ,Motion layout \\nArchitectural Pattern  MVC \\nIDE  Android Studio \\nServices  AWS bucket s3 service, Bitbucket  \\n \\nPROJECT DETAILS \\n \\nProject 1 \\nBoomi is the name of the our project which is e-commerce online store. We’d developed and maintains by react \\ncode. \\nRole  Front End  Developer \\nProject  E-commerce \\nTechnology  Android \\nRole Played  ●  Developing applications using Technologies like React and HTML. \\n●  Involved in Requirement Gathering. \\n●  Resolving client given queries. \\n●  Communicating with the team and adding features. \\n●  Performed Restful API services using volley. \\n \\n \\n \\nPage | 1  \\n '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readpdf(\"./Resumes/React_Developer/Reactjs Developer_Prabakaran_Musquare Technologies.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c64d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce4f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d6bce71",
   "metadata": {},
   "outputs": [],
   "source": [
    " pdf=pdfplumber.open(\"./Resumes/React_Developer/Reactjs Developer_Prabakaran_Musquare Technologies.pdf\")\n",
    "pages = pdf.pages[0]\n",
    "text=pages.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa22453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a032d3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     \\nName: M. Prabakaran \\nTitle: UI Developer \\n \\nPROFESSIONAL SUMMARY \\n \\n●  2.4+ years of Professional IT experience as a software developer having knowledge on different UI based \\nApplication. \\n●  Hands on experience in HTML, CSS, JS, ReactJS.   \\n●  Hands on experience in handling UI interaction, Design methodology. \\n●  Handling In-App purchase, uploading and maintaining apps in play store. \\n●  Hands on experience with customization over base-product depends on client requirement.  \\n●  Cohesive team worker, having strong analytical, problem solving and interpersonal skills. \\n \\nEDUCATION \\n \\n●  Completed on 2017 Bachelor of Technology (ECE), PRIST University, Tamil Nadu. \\n●  Completed on 2012 Higher Secondary, Mount Park Hr Sec School, Thiyagadurgam, Tamil Nadu. \\n●  Completed on 2010 SSLC, Krishnasamy Hr Sec School, Cuddalore, Tamil Nadu. \\n \\nSKILLS \\n \\nOperating System  Windows, Unix,Mac \\nProgramming Languages  HTML, CSS, JS, React \\nMobile App Development  Windows and Android Apps \\nDesigning  Constatinant layout ,Motion layout \\nArchitectural Pattern  MVC \\nIDE  Android Studio \\nServices  AWS bucket s3 service, Bitbucket  \\n \\nPROJECT DETAILS \\n \\nProject 1 \\nBoomi is the name of the our project which is e-commerce online store. We’d developed and maintains by react \\ncode. \\nRole  Front End  Developer \\nProject  E-commerce \\nTechnology  Android \\nRole Played  ●  Developing applications using Technologies like React and HTML. \\n●  Involved in Requirement Gathering. \\n●  Resolving client given queries. \\n●  Communicating with the team and adding features. \\n●  Performed Restful API services using volley. \\n \\n \\n \\nPage | 1  \\n '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e68df62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_11=[]\n",
    "li_11.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f86aa958",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1fd2423",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"resume\"]=li_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "010d5d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nName: M. Prabakaran \\nTitle: UI Develop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              resume\n",
       "0       \\nName: M. Prabakaran \\nTitle: UI Develop..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8f34c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "lz = WordNetLemmatizer()\n",
    "for i in range(dataframe.shape[0]):\n",
    "    review = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n",
    "        \" \",\n",
    "        dataframe[\"resume\"].iloc[i],\n",
    "    )\n",
    "    review = re.sub(r\"[0-9]+\", \" \", review) # Remove Numbers\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [ lz.lemmatize(word) for word in review if word not in STOP_WORDS]\n",
    "    review = \" \".join(review)\n",
    "    clean.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfe00d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"Clean\"] = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "260df22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m prabakaran title ui developer professional summary year professional experience software developer having knowledge different ui based application hand experience html cs j reactjs hand experience handling ui interaction design methodology handling app purchase uploading maintaining apps play store hand experience customization base product depends client requirement cohesive team worker having strong analytical problem solving interpersonal skill education completed bachelor technology ece prist university tamil nadu completed higher secondary mount park hr sec school thiyagadurgam tamil nadu completed sslc krishnasamy hr sec school cuddalore tamil nadu skill operating system window unix mac programming language html cs j react mobile app development window android apps designing constatinant layout motion layout architectural pattern mvc ide android studio service aws bucket s service bitbucket project detail project boomi project e commerce online store d developed maintains react code role end developer project e commerce technology android role played developing application technology like react html involved requirement gathering resolving client given query communicating team adding feature performed restful api service volley page'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"Clean\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ecc27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Information about ./Resumes/React_Developer/Reactjs Developer_Prabakaran_Musquare Technologies.pdf: \n",
      "\n",
      "    Author: None\n",
      "    Creator: Microsoft® Word for Microsoft 365\n",
      "    Producer: Microsoft® Word for Microsoft 365\n",
      "    Subject: None\n",
      "    Title: None\n",
      "    Number of pages: 2\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# extract_doc_info.py\n",
    "\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "def extract_information(pdf_path):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        pdf = PdfFileReader(f)\n",
    "        information = pdf.getDocumentInfo()\n",
    "        number_of_pages = pdf.getNumPages()\n",
    "    txt = f\"\"\"\n",
    "    Information about {pdf_path}: \n",
    "\n",
    "    Author: {information.author}\n",
    "    Creator: {information.creator}\n",
    "    Producer: {information.producer}\n",
    "    Subject: {information.subject}\n",
    "    Title: {information.title}\n",
    "    Number of pages: {number_of_pages}\n",
    "    \"\"\"\n",
    "\n",
    "    print(txt)\n",
    "    return information\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path =\"./Resumes/React_Developer/Reactjs Developer_Prabakaran_Musquare Technologies.pdf\"\n",
    "    extract_information(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908952e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242ff8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f3ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224e51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./train_data/'\n",
    "all_files=glob.glob(path + \"/*.docx\")\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtxt(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "li=[]\n",
    "for filename in all_files:\n",
    "    dummy_1=readtxt(filename)\n",
    "    li.append(dummy_1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26bcd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30958f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586486e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in all_files:\n",
    "    #print(files)\n",
    "    dataframe[\"resume\"]=li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398fe8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "lz = WordNetLemmatizer()\n",
    "for i in range(dataframe.shape[0]):\n",
    "    review = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n",
    "        \" \",\n",
    "        dataframe[\"resume\"].iloc[i],\n",
    "    )\n",
    "    review = re.sub(r\"[0-9]+\", \" \", review) # Remove Numbers\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [ lz.lemmatize(word) for word in review if word not in STOP_WORDS]\n",
    "    review = \" \".join(review)\n",
    "    clean.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d462b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"Clean\"] = clean  #Clean_Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bed7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370cfc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"Clean_Resume\"]=dataframe[\"Clean\"].apply(uniquify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"Clean_Resume\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cad98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ec1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe568dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_set=[\"SQL\",\" C\", \"EIB\",\n",
    "           \"Oracle\",\"PeopleSoft\",\"AWSRedshift\",\"FCM\",\n",
    "           \"HTML\" ,  \"React\",\"JAVA\"]  #\"XML\"\"XSLT\"\"MSBI\",\"Python\",\"JavaScript\",\"Tableau\" ,\"MariaDB\",\"TSQL\",\"MSExcel\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5224fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(skill_set)):\n",
    "    skill_set[i] = skill_set[i].lower()\n",
    "print(skill_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75738746",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(dataframe.shape[0]):\n",
    "    resume =nlp(dataframe[\"Clean_Resume\"][i]) \n",
    "    noun_chunks = resume.noun_chunks\n",
    "    def extract_skills(resume_text):\n",
    "        nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "        tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "    # extract values\n",
    "        skills = skill_set\n",
    "    \n",
    "        skillset = []\n",
    "    \n",
    "    # check for one-grams\n",
    "        for token in tokens:\n",
    "            if token in skills:\n",
    "                skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams\n",
    "        for token in noun_chunks:\n",
    "            token = token.text.lower().strip()\n",
    "            if token in skills:\n",
    "                skillset.append(token)\n",
    "    \n",
    "        return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee00f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80615e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Skill\"]=dataframe[\"Clean_Resume\"].apply(extract_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68394858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(dataframe.shape[0]):\n",
    "    resume =nlp(dataframe[\"Clean_Resume\"][i]) \n",
    "    noun_chunks = resume.noun_chunks\n",
    "    def extract_skills_1(resume_text):\n",
    "        nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "        tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "    # extract values\n",
    "        skills = skill_set\n",
    "    \n",
    "        skillset = []\n",
    "        string =\"\"\n",
    "        \n",
    "    \n",
    "    # check for one-grams\n",
    "        for token in tokens:\n",
    "            if token in skills:\n",
    "                skillset.append(token)\n",
    "                \n",
    "        string=string+str(skillset.pop())\n",
    "    # check for bi-grams and tri-grams\n",
    "        #for token in noun_chunks:\n",
    "            #token = token.text.lower()\n",
    "            #if token in skills:\n",
    "                #skillset.append(token)\n",
    "                #string=string+str(skillset.pop())\n",
    "    \n",
    "        return  string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a636609",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Skill_one\"]=dataframe[\"Clean_Resume\"].apply(extract_skills_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7ef83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Displaying the distinct categories of resume and the number of records belonging to each category -\")\n",
    "print (df['Skill_one'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"The distinct categories of resumes\")\n",
    "plt.xticks(rotation=90)\n",
    "sns.countplot(y=\"Skill_one\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "targetCounts = df['Skill_one'].value_counts()\n",
    "targetLabels  = df['Skill_one'].unique()\n",
    "# Make square figures and axes\n",
    "plt.figure(1, figsize=(25,25))\n",
    "the_grid = GridSpec(2, 2)\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('plasma')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1,9)]\n",
    "plt.subplot(the_grid[0, 1], aspect=1, title='CATEGORY DISTRIBUTION')\n",
    "\n",
    "source_pie = plt.pie(targetCounts, labels=targetLabels, autopct='%1.1f%%', shadow=True, colors=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanResume(resumeText):\n",
    "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
    "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
    "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
    "    resumeText = re.sub(r\"[0-9]+\", \" \", resumeText)\n",
    "    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    return resumeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "oneSetOfStopWords = set(stopwords.words('english')+['``',\"''\"])\n",
    "totalWords =[]\n",
    "Sentences = dataframe[\"Clean_Resume\"]\n",
    "cleanedSentences = \"\"\n",
    "for i in range(len(dataframe[\"resume\"])):\n",
    "    cleanedText = cleanResume(Sentences[i])\n",
    "    cleanedSentences += cleanedText\n",
    "    requiredWords = nltk.word_tokenize(cleanedText)\n",
    "    for word in requiredWords:\n",
    "        if word not in oneSetOfStopWords and word not in string.punctuation:\n",
    "            totalWords.append(word)\n",
    "    \n",
    "wordfreqdist = nltk.FreqDist(totalWords)\n",
    "mostcommon = wordfreqdist.most_common(50)\n",
    "print(mostcommon)\n",
    "\n",
    "wc = WordCloud().generate(cleanedSentences)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordBarGraphFunction_1(df,column,title):\n",
    "    topic_words = [ z.lower() for y in\n",
    "                       [ x.split() for x in df[column] if isinstance(x, str)]\n",
    "                       for z in y]\n",
    "    word_count_dict = dict(Counter(topic_words))\n",
    "    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n",
    "    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sns.barplot(x=np.arange(20),y= [word_count_dict[w] for w in reversed(popular_words_nonstop[0:20])])\n",
    "    plt.xticks([x + 0.5 for x in range(20)], reversed(popular_words_nonstop[0:20]),rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "wordBarGraphFunction_1(dataframe,\"Clean_Resume\",\"Most frequent Words \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad87af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data=pd.DataFrame()\n",
    "resume_data[\"Resume\"]=dataframe[\"Clean_Resume\"]\n",
    "resume_data[\"category\"]=df[\"Skill_one\"]\n",
    "resume_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_encoder=LabelEncoder()\n",
    "resume_data[\"Encoded_Skill\"]=le_encoder.fit_transform(resume_data[\"category\"])\n",
    "resume_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "requiredText = resume_data[\"Resume\"].values\n",
    "requiredTarget = resume_data[\"Encoded_Skill\"].values\n",
    "\n",
    "'''\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=2000)\n",
    "word_vectorizer.fit(requiredText)\n",
    "WordFeatures = word_vectorizer.transform(requiredText)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=2000)\n",
    "char_vectorizer.fit(requiredText)\n",
    "CharFeatures = char_vectorizer.transform(requiredText)\n",
    "totalFeatures = hstack([WordFeatures, CharFeatures])\n",
    "'''\n",
    "#word_vectorizer = TfidfVectorizer(smooth_idf=True,  analyzer='word',token_pattern=r'\\w{1,}',\n",
    "    #sublinear_tf=True,\n",
    "    #stop_words='english',\n",
    "    #max_features=1500)\n",
    "#word_vectorizer.fit(requiredText)\n",
    "#WordFeatures = word_vectorizer.transform(requiredText)\n",
    "\n",
    "\n",
    "word_vectorizer = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',max_df=0.5,\n",
    "    min_df=1,\n",
    "            ngram_range=(1, 1), stop_words = 'english')\n",
    "word_vectorizer.fit(requiredText)\n",
    "WordFeatures = word_vectorizer.transform(requiredText)\n",
    "\n",
    "print (\"Feature completed .....\")\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,stratify=requiredTarget,random_state=None, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#\n",
    "#min_df=3,  max_features=None, \n",
    "            #strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            #ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            #stop_words = 'english'\n",
    "#ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            #ngram_range=(1, 3), stop_words = 'english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c464cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score,classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "#init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258270ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_prediction = knn.predict(X_test)\n",
    "knn_score = knn.score(X_test, y_test)\n",
    "print(\"KNN Classification Train Accuracy: {}%\".format(round(knn.score(X_train,y_train)*100,2)))\n",
    "print(\"KNN Classification Test Accuracy: {}%\".format(round(knn.score(X_test,y_test)*100,2)))\n",
    "knn_cm = confusion_matrix(y_test, knn_prediction)\n",
    "\n",
    "print('Classification Report:\\n')\n",
    "\n",
    "print(classification_report(y_test, knn_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef35063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Best K Value\n",
    "\n",
    "score_list = []\n",
    "for each in range(1,30):\n",
    "    knn2 =KNeighborsClassifier(n_neighbors=each, \n",
    "                                                   weights='uniform',p=2,metric='minkowski')\n",
    "    knn2.fit(X_train, y_train)\n",
    "    score_list.append(knn2.score(X_test, y_test))\n",
    "plt.plot(range(1,30), score_list)\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07525009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c54bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print('Accuracy of MultinomialNB Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of MultinomialNB Classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\" % (clf, metrics.classification_report(y_test, prediction)))\n",
    "nb_score = clf.score(X_test, y_test)\n",
    "nb_cm = confusion_matrix(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e247b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e489b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =RandomForestClassifier(n_estimators=200,criterion='entropy',max_depth=5,max_features='auto',random_state=None,\n",
    " class_weight=\"balanced\")\n",
    "rf.fit(X_train, y_train)\n",
    "rf_prediction = rf.predict(X_test)\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print(\"Random Forest Classification Train Accuracy: {}%\".format(round(rf.score(X_train,y_train)*100,2)))\n",
    "print(\"Random Forest Classification Test Accuracy: {}%\".format(round(rf.score(X_test,y_test)*100,2)))\n",
    "rf_cm = confusion_matrix(y_test, rf_prediction)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, rf_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33776322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search_ = GridSearchCV(estimator =rf,param_grid = {'criterion':['entropy','gini'],\n",
    "                                                                'max_depth':[1,2,3,4,5,6,7,8,9,10]},\n",
    "                              cv=5)\n",
    "grid_search_.fit(WordFeatures,requiredTarget)\n",
    "print(grid_search_.best_params_)\n",
    "print(grid_search_.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm =SVC(C=5,kernel='linear',degree=3,gamma='auto',class_weight ='balanced')\n",
    "svm.fit(X_train, y_train)\n",
    "svm_prediction = svm.predict(X_test)\n",
    "svm_score = svm.score(X_test, y_test)\n",
    "print(\"SVM Classification Train Accuracy: {}%\".format(round(svm.score(X_train,y_train)*100,2)))\n",
    "print(\"SVM Classification Test Accuracy: {}%\".format(round(svm.score(X_test,y_test)*100,2)))\n",
    "svm_cm = confusion_matrix(y_test, svm_prediction)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "print(classification_report(y_test, svm_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e747c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4741d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15b8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_set=[\"SQL Server\",\"SQL\", \"T-SQL\",\"SAS\", \"R\", \"Python\",\"MariaDB\",\" AWS RDS (Athena)\",\"MS Excel\", \"Tableau\",\"XML\", \"XSLT\", \"EIB\"\n",
    "           ,\"Oracle\",\"PL/SQL\",\"PeopleSoft HCM\",\"SQL Developer\",\"SQL SERVER DEVELOPER\",\"AWS Redshift\",\"HCM\",\"FCM\",\"Microsoft Business Intelligence Tools(MSBI)\",\n",
    "           \"HTML\", \"CSS3\", \"XML\", \"JavaScript\", \"JSON\", \"React JS\", \"Node.js\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa23cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sql server', 'sql', 't-sql', 'sas', 'r', 'python', 'mariadb', ' aws rds (athena)', 'ms excel', 'tableau', 'xml', 'xslt', 'eib', 'oracle', 'pl/sql', 'peoplesoft hcm', 'sql developer', 'sql server developer', 'aws redshift', 'hcm', 'fcm', 'microsoft business intelligence tools(msbi)', 'html', 'css3', 'xml', 'javascript', 'json', 'react js', 'node.js']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(skill_set)):\n",
    "    skill_set[i] = skill_set[i].lower()\n",
    "print(skill_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34babbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ec54f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
